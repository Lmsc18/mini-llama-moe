{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from model.modules.linear import Linear\n",
    "from model.modules.swiglu import SWIGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TSConfig:\n",
    "    seq_len: int = 256 \n",
    "    vocab_size: int = 10000\n",
    "    num_layer: int = 4\n",
    "    num_heads: int = 16\n",
    "    d_model: int = 512\n",
    "    d_ff:int=1344\n",
    "    num_experts:int=8\n",
    "    k:int=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=TSConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(32,config.seq_len,config.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeLayer(nn.Module):\n",
    "    def __init__(self, config, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.d_ff = config.d_ff\n",
    "        self.k = config.k\n",
    "        self.n_exp = config.num_experts\n",
    "\n",
    "        self.experts = nn.ModuleList(\n",
    "            [SWIGLU(config) for _ in range(config.num_experts)]\n",
    "        )\n",
    "        self.shared_expert = SWIGLU(config)\n",
    "        self.gate = Linear(config.d_model, config.num_experts)\n",
    "        self.aux_loss=0.0\n",
    "        self.z_loss=0.0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        B, T, D = inputs.shape\n",
    "        x = inputs.view(-1, D)  # [B*T, D]\n",
    "        logits = self.gate(x)  # [B*T, n_exp]\n",
    "        topk_scores, topk_indices = torch.topk(logits, self.k, dim=-1)  # [B*T, k]\n",
    "        probs = F.softmax(topk_scores, dim=-1, dtype=torch.float).type_as(inputs)  # [B*T, k]\n",
    "\n",
    "        output = torch.zeros_like(x)\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            batch_pos, expert_pos = torch.where(topk_indices == i)\n",
    "            if batch_pos.numel() == 0:\n",
    "                continue\n",
    "            selected_inputs = x[batch_pos]\n",
    "            expert_output = expert(selected_inputs)\n",
    "            weight = probs[batch_pos, expert_pos].unsqueeze(-1)\n",
    "            output[batch_pos] += weight * expert_output\n",
    "\n",
    "        # Shared expert is always used\n",
    "        output += self.shared_expert(x)\n",
    "\n",
    "        # Compute losses\n",
    "        aux_loss = self.compute_aux_loss(probs, topk_indices, self.n_exp)\n",
    "        z_loss = self.compute_router_z_loss(logits)\n",
    "        self.aux_loss=aux_loss\n",
    "        self.z_loss=z_loss\n",
    "        return output.view(B, T, D)\n",
    "    @staticmethod\n",
    "    def compute_aux_loss(expert_probs: torch.Tensor, indices: torch.Tensor, n_exp: int):\n",
    "        \"\"\"\n",
    "        Switch Transformer auxiliary loss (eq 4-6)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            one_hot = F.one_hot(indices, num_classes=n_exp).sum(dim=1).float()  # [B*T, n_exp]\n",
    "            tokens_per_expert = one_hot.mean(dim=0)  # [n_exp]\n",
    "        prob_per_expert = expert_probs.new_zeros(expert_probs.size(0), n_exp).scatter_add(\n",
    "            1, indices, expert_probs\n",
    "        ).mean(dim=0)  # [n_exp]\n",
    "        return n_exp * torch.sum(prob_per_expert * tokens_per_expert)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_router_z_loss(logits: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ST-MoE router z-loss (eq 5 in ST-MoE)\n",
    "        \"\"\"\n",
    "        return torch.mean(torch.logsumexp(logits, dim=-1) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe=MoeLayer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=moe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moe.aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aux_loss': tensor(8., grad_fn=<MulBackward0>),\n",
       " 'z_loss': tensor(955.0957, grad_fn=<MeanBackward0>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
