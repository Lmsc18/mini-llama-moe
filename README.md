# mini-llama-moe
MOE version of mini-llama.
The MOE has 8 experts among which 2 routed experts and 1 shared expert are activated.
The implementation also uses auxiliary load balancing loss and z-loss.
